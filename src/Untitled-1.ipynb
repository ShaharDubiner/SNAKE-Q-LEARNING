{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Reading df ###\n",
      "type\n",
      "bad        9495\n",
      "good        450\n",
      "neutral      55\n",
      "Name: count, dtype: int64\n",
      "### Generate ngram dict ###\n",
      "border_good: 1 / 409\n",
      " border_bad: 28 / 9209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 9989/10000 [01:35<00:00, 101.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9999, clusters_count size: 7328040, temp_dict size: 909121\n",
      "Size in RAM - clusters_count: 5.370948381721973 Gb, temp_dict: 1.0965395495295525 Gb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [01:57<00:00, 84.97it/s]\n",
      "100%|██████████| 909121/909121 [01:22<00:00, 11085.35it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###### Time of dict_by_clusters  199.85746693611145  Len of calc_good  857546  ###\n",
      "\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 30] Read-only file system: 'ssn_test_calc_good_v3.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 848\u001b[0m\n\u001b[1;32m    844\u001b[0m base_query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124min:anywhere (\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msocial security number\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mssn\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoc sec num\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSocial Security\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSocial Sec\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSoc Sec\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSoc Security Number\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSocial Security No\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSS No\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSS Num\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSS Number\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSocial Security Administration\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSSA\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSocial Sec Admin\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSocial Security Admin\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSoc Sec Admin\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSoc Security Administration\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSoc Security Admin\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSS Administration\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    846\u001b[0m x_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 848\u001b[0m df, calc_good, calc_bad, group_minus, groups, query, out_good_set \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline_of_query_creating\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mngram_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mminus_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 727\u001b[0m, in \u001b[0;36mpipeline_of_query_creating\u001b[0;34m(path_params, ngram_params, minus_params, group_params, base_query, x_test)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m###### Time of dict_by_clusters \u001b[39m\u001b[38;5;124m'\u001b[39m, time\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;241m-\u001b[39mtic, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m Len of calc_good \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mlen\u001b[39m(calc_good), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m ###\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    725\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[0;32m--> 727\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcalc_good\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    728\u001b[0m     pickle\u001b[38;5;241m.\u001b[39mdump(calc_good, f)\n\u001b[1;32m    730\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcalc_bad\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/IPython/core/interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m     )\n\u001b[0;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 30] Read-only file system: 'ssn_test_calc_good_v3.pkl'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import string\n",
    "import regex as re\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm  # Changed this line\n",
    "import csv\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.close('all')\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "pd.set_option('display.max_rows', 250)\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed\n",
    "from math import ceil\n",
    "import time\n",
    "\n",
    "import gc\n",
    "import sys\n",
    "\n",
    "forbidden_words = ['abbo ', 'abo ', 'abortion', 'abuse', 'addict', 'addicts', 'adult', 'alla ', 'allah ', 'alligatorbait', 'amateur', 'anal ', 'analannie', 'analsex', 'angry', 'anus ', 'areola', 'argie ', 'aroused', 'arse ', 'arsehole', 'asian ', 'ass ', 'assassin', 'assassinate', 'assassination', 'assault', 'assbagger', 'assblaster', 'assclown', 'asscowboy', 'asses ', 'assfuck', 'assfucker', 'asshat', 'asshole', 'assholes', 'asshore', 'assjockey', 'asskiss', 'asskisser', 'assklown', 'asslick', 'asslicker', 'asslover', 'assman ', 'assmonkey', 'assmunch', 'assmuncher', 'asspacker', 'asspirate', 'asspuppies', 'assranger', 'asswhore', 'asswipe', 'athletesfoot', 'attack', 'babe ', 'babies', 'backdoor', 'backdoorman', 'backseat', 'badfuck', 'balllicker', ' balls ', 'ballsack', 'banging', 'baptist', 'barelylegal', 'barf ', 'barface', 'barfface', 'bast ', 'bastard', 'bazongas', 'bazooms', 'beaner', 'beastality', 'beastial', 'beastiality', 'beatoff', 'beat-off', 'beatyourmeat', 'beaver', 'bestial', 'bestiality', 'biatch', 'bicurious', 'bigass', 'bigbastard', 'bigbutt', 'bisexual', 'bi-sexual', 'bitch', 'bitcher', 'bitches', 'bitchez', 'bitchin', 'bitching', 'bitchslap', 'bitchy', 'biteme', 'blackman', 'blackout', 'blacks', 'blind', 'blow ', 'blowjob', 'boang', 'bogan', 'bohunk', 'bollick', 'bollock', 'bombers', 'bombing', 'bombs', 'bomd', 'bondage', 'boner', 'bong', 'boob', 'boobies', 'boobs', 'booby', 'boody', 'boong', 'boonga', 'boonie', 'booty', 'bootycall', 'bountybar', 'bra ', 'brea5t', 'breast', 'breastjob', 'breastlover', 'breastman', 'brothel', 'bugger', 'buggered', 'buggery', 'bullcrap', 'bulldike', 'bulldyke', 'bullshit', 'bumblefuck', 'bumfuck', 'bunga ', 'bunghole', 'buried', 'butchbabes', 'butchdike', 'butchdyke', 'butt ', 'buttbang', 'butt-bang', 'buttface', 'buttfuck', 'butt-fuck', 'buttfucker', 'butt-fucker', 'buttfuckers', 'butt-fuckers', 'butthead', 'buttman', 'buttmunch', 'buttmuncher', 'buttpirate', 'buttplug', 'buttstain', 'byatch', 'cacker', 'cameljockey', 'cameltoe', 'canadian', 'carpetmuncher', 'carruth', 'cemetery', 'chav ', 'cherrypopper', 'chickslick', \"children's\", 'chinaman', 'chinamen', 'chinese', 'chink ', 'chinky', 'choad', 'chode', 'cigs ', 'clamdigger', 'clamdiver', 'clit ', 'clitoris', 'clogwog', 'cocaine', 'cock ', 'cockblock', 'cockblocker', 'cockcowboy', 'cockfight', 'cockhead', 'cockknob', 'cocklicker', 'cocklover', 'cocknob', 'cockqueen', 'cockrider', 'cocksman', 'cocksmith', 'cocksmoker', 'cocksucer', 'cocksuck', 'cocksucked', 'cocksucker', 'cocksucking', 'cocktail', 'cocktease', 'cocky', 'cohee', 'coitus', 'commie', 'communist', 'condom', 'conservative', 'conspiracy', 'coolie', 'cooly', 'coon ', 'coondog', 'copulate', 'cornhole', 'corruption', 'cra5h', 'crabs', 'crack', 'crackpipe', 'crackwhore', 'crack-whore', 'crap ', 'crapola', 'crapper', 'crappy', 'crash ', 'crotch ', 'crotchjockey', 'crotchmonkey', 'crotchrot', 'cum ', 'cumbubble', 'cumfest', 'cumjockey', 'cumm', 'cummer', 'cumming', 'cumquat', 'cumqueen', 'cumshot', 'cunilingus', 'cunillingus', 'cunn ', 'cunnilingus', 'cunntt', 'cunt ', 'cunteyed', 'cuntfuck', 'cuntfucker', 'cuntlick', 'cuntlicker', 'cuntlicking', 'cuntsucker', 'cybersex', 'cyberslimer', 'dago ', 'dahmer', 'dammit', 'damn ', 'damnation', 'damnit', 'darkie', 'darky', 'datnigga', 'dead ', 'deapthroat', 'death', 'deepthroat', 'defecate', 'dego ', 'demon', 'desire', 'destroy', 'deth ', 'devilworshipper', 'dick ', 'dickbrain', 'dickforbrains', 'dickhead', 'dickless', 'dicklick', 'dicklicker', 'dickman', 'dickwad', 'dickweed', 'diddle', 'dike ', 'dildo', 'dingleberry', 'dink ', 'dipshit', 'dipstick', 'dirty ', 'disease', 'diseases', 'disturbed', 'dix ', 'dixiedike', 'dixiedyke', 'doggiestyle', 'doggystyle', 'dong ', 'doodoo', 'doo-doo', 'doom ', 'dope ', 'dragqueen', 'dragqween', 'dripdick', 'drug ', 'drunk', 'drunken', 'dumb ', 'dumbass', 'dumbbitch', 'dumbfuck', 'dyefly', 'dyke ', 'easyslut', 'eatballs', 'eatpussy', 'ecstacy', 'ejaculate', 'ejaculated', 'ejaculating', 'ejaculation', 'enema', 'enemy', 'erect', 'erection', ' ero ', 'escort', 'ethiopian', 'ethnic', 'european', ' evl ', 'excrement', 'execute ', 'executed', 'execution', 'executioner', 'explosion', 'facefucker', 'faeces', ' fag ', 'fagging', 'faggot', 'fagot', 'failed', 'failure', 'fairies', 'fannyfucker', 'fart ', 'farted', 'farting', 'farty', 'fastfuck', 'fat ', 'fatah', 'fatass', 'fatfuck', 'fatfucker', 'fatso', 'fckcum', 'feces', 'felatio', 'felch', 'felcher', 'felching', 'fellatio', 'feltch', 'feltcher', 'feltching', 'fetish', 'filipina', 'filipino', 'fingerfood', 'fingerfuck', 'fingerfucked', 'fingerfucker', 'fingerfuckers', 'fingerfucking', 'fister', 'fistfuck', 'fistfucked', 'fistfucker', 'fistfucking', 'fisting', 'flange', 'flasher', 'flatulence', 'floo ', 'flydie', 'flydye', 'fok ', 'fondle', 'footaction', 'footfuck', 'footfucker', 'footlicker', 'footstar', 'foreskin', 'forni', 'fornicate', 'foursome', 'fourtwenty', 'fraud', 'freakfuck', 'freakyfucker', 'freefuck', 'fubar', 'fucck', 'fuck', 'fucka', 'fuckable', 'fuckbag', 'fuckbuddy', 'fucked', 'fuckedup', 'fucker', 'fuckers', 'fuckface', 'fuckfest', 'fuckfreak', 'fuckfriend', 'fuckhead', 'fuckher', 'fuckin', 'fuckina', 'fucking', 'fuckingbitch', 'fuckinnuts', 'fuckinright', 'fuckit', 'fuckknob', 'fuckme', 'fuckmehard', 'fuckmonkey', 'fuckoff', 'fuckpig', 'fucks', 'fucktard', 'fuckwhore', 'fuckyou', 'fudgepacker', 'fugly', ' fuk ', 'fuks', 'funeral', 'funfuck', 'fungus', 'fuuck', 'gangbang', 'gangbanged', 'gangbanger', 'gangsta', 'gatorbait', 'gaymuthafuckinwhore', 'gaysex', 'geez', 'geezer', 'geni', 'genital', 'german', 'getiton', 'ginzo', 'gipp', 'girls', 'givehead', 'glazeddonut', 'godammit', 'goddamit', 'goddammit', 'goddamn', 'goddamned', 'goddamnes', 'goddamnit', 'goddamnmuthafucker', 'goldenshower', 'gonorrehea', 'gonzagas', 'gook', 'gotohell', 'greaseball', 'gringo', 'groe ', 'gross ', 'grostulation', 'gubba', 'gummer', 'gypo', 'gypp', 'gyppie', 'gyppo', 'gyppy', 'hamas', 'handjob', 'hapa', 'harder', 'hardon', 'harem', 'headfuck', 'headlights', 'hebe ', 'heeb ', 'henhouse', 'heroin', 'herpes', 'heterosexual', 'hijack', 'hijacker', 'hijacking', 'hillbillies', 'hindoo', 'hiscock', 'hitler', 'hitlerism', 'hitlerist', 'hiv ', 'hobo', 'hodgie', 'hoes', 'holestuffer', 'homicide', 'homo', 'homobangers', 'homosexual', 'honger', 'honk', 'honkers', 'honkey', 'honky', 'hook', 'hooker', 'hookers', 'hooters', 'hore', 'hork', 'horn', 'horney', 'horniest', 'horny', 'horseshit', 'hosejob', 'hoser', 'hostage', 'hotdamn', 'hotpussy', 'hottotrot', 'husky', 'hussy', 'hustler', 'hymen', 'hymie', 'iblowu', 'idiot', 'ikey', 'illegal', 'incest', 'insest', 'intercourse', 'interracial', 'intheass', 'inthebuff', 'jackass', 'jackoff', 'jackshit', 'jacktheripper', 'japcrap', 'jebus', 'jeez', 'jerkoff', 'jesus', 'jesuschrist', 'jiga', 'jigaboo', 'jigg', 'jigga', 'jiggabo', 'jigger', 'jiggy', 'jihad', 'jijjiboo', 'jimfish', 'jism', 'jizim', 'jizjuice', 'jizm', 'jizz', 'jizzim', 'jizzum', 'joint', 'juggalo', 'jugs', 'junglebunny', 'kaffer', 'kaffir', 'kaffre', 'kafir', 'kanake', 'kigger', 'kike', 'kill', 'killed', 'killer', 'killing', 'kills', 'kink', 'kinky', 'kissass', 'knife', 'knockers', 'kock', 'kondum', 'koon', 'kotex', 'krap', 'krappy', 'kraut', 'kum ', 'kumbubble', 'kumbullbe', 'kummer', 'kumming', 'kumquat', 'kums', 'kunilingus', 'kunnilingus', 'kunt', 'kyke', 'lactate', 'laid', 'lapdance', 'latin', 'lesbain', 'lesbayn', 'lesbian', 'lesbin', 'lesbo', 'lezbe', 'lezbefriends', 'lezbo', 'lezz', 'lezzo', 'liberal', 'libido', 'licker', 'lickme', 'limey', 'limpdick', 'limy', 'lingerie', 'liquor', 'livesex', 'loadedgun', 'lolita', 'looser', 'loser', 'lotion', 'lovebone', 'lovegoo', 'lovegun', 'lovejuice', 'lovemuscle', 'lovepistol', 'loverocket', 'lowlife', 'lubejob', 'lucifer', 'luckycammeltoe', 'lugan', 'lynch', 'macaca', 'mafia ', 'magicwand', 'manhater', 'manpaste', 'marijuana', 'mastabate', 'mastabater', 'masterbate', 'masterblaster', 'mastrabator', 'masturbate', 'masturbating', 'mattressprincess', 'meatbeatter', 'meatrack', 'meth ', 'mexican', 'mgger', 'mggor', 'mickeyfinn', 'mideast', 'milf', 'minority', 'mockey', 'mockie', 'mocky', 'mofo', 'moky', 'moles', 'molest', 'molestation', 'molester', 'molestor', 'moneyshot', 'mooncricket', 'mormon', 'moron', 'moslem', 'mosshead', 'mothafuck', 'mothafucka', 'mothafuckaz', 'mothafucked', 'mothafucker', 'mothafuckin', 'mothafucking', 'mothafuckings', 'motherfuck', 'motherfucked', 'motherfucker', 'motherfuckin', 'motherfucking', 'motherfuckings', 'motherlovebone', 'muff', 'muffdive', 'muffdiver', 'muffindiver', 'mufflikcer', 'mulatto', 'muncher', 'munt', 'murder', 'murderer', 'naked', 'narcotic', 'nasty', 'nastybitch', 'nastyho', 'nastyslut', 'nastywhore', 'nazi', 'necro', 'negro', 'negroes', 'negroid', \"negro's\", 'niger', 'nigerian', 'nigerians', 'nigg', 'nigga', 'niggah', 'niggaracci', 'niggard', 'niggarded', 'niggarding', 'niggardliness', \"niggardliness's\", 'niggardly', 'niggards', \"niggard's\", 'niggaz', 'nigger', 'niggerhead', 'niggerhole', 'niggers', \"nigger's\", 'niggle', 'niggled', 'niggles', 'niggling', 'nigglings', 'niggor', 'niggur', 'niglet', 'nignog', 'nigr', 'nigra', 'nigre', 'nip ', 'nipple', 'nipplering', 'nittit', 'nlgger', 'nlggor', 'nofuckingway', 'nook', 'nookey', 'nookie', 'noonan', 'nooner', 'nude', 'nudger', 'nuke', 'nutfucker', 'nymph', 'ontherag', 'oral', 'orga ', 'orgasim', 'orgasm', 'orgies', 'orgy', 'osama', 'paki', 'palesimian', 'palestinian', 'pansies', 'pansy', 'panti', 'panties', 'payo', 'pearlnecklace', 'peck', 'pecker', 'peckerwood', 'pee ', 'peehole', 'pee-pee', 'peepshow', 'peepshpw', 'pendy', 'penetration', 'peni5', 'penile', 'penis', 'penises', 'penthouse', 'perv', 'phonesex', 'phuk', 'phuked', 'phuking', 'phukked', 'phukking', 'phungky', 'phuq', 'pi55', 'picaninny', 'piccaninny', 'pickaninny', 'piker', 'pikey', 'piky', 'pimp', 'pimped', 'pimper', 'pimpjuic', 'pimpjuice', 'pimpsimp', 'pindick', 'piss', 'pissed', 'pisser', 'pisses', 'pisshead', 'pissin', 'pissing', 'pissoff', 'pistol', 'pixie', 'pixy', 'playboy', 'playgirl', 'pocha', 'pocho', 'pocketpool', 'pohm', 'polack', 'pom', 'pommie', 'pommy', 'poon', 'poontang', 'poop', 'pooper', 'pooperscooper', 'pooping', 'poorwhitetrash', 'popimp', 'porchmonkey', 'porn', 'pornflick', 'pornking', 'porno', 'pornography', 'pornprincess', 'poverty', 'premature', 'prick', 'prickhead', 'primetime', 'propaganda', 'prostitute', 'protestant', 'pu55i', 'pu55y', 'pube', 'pubic', 'pubiclice', 'pud', 'pudboy', 'pudd', 'puddboy', 'puke', 'puntang', 'purinapricness', 'puss', 'pussie', 'pussies', 'pussy', 'pussycat', 'pussyeater', 'pussyfucker', 'pussylicker', 'pussylips', 'pussylover', 'pussypounder', 'pusy', 'quashie', 'queef', 'queer', 'quickie', 'quim', 'ra8s', 'racial', 'racist', 'radical', 'radicals', 'raghead', 'randy', 'rape ', 'raped', 'raper ', 'rapist', 'rearend', 'rearentry', 'rectum', 'redlight', 'redneck', 'reefer', 'reestie', 'refugee', 'reject', 'remains', 'rentafuck', 'republican', 'rere', 'retard', 'retarded', 'ribbed', 'rigger', 'rimjob', 'rimming', 'roach', 'robber', 'roundeye', 'rump', 'russki', 'russkie', 'sadis', 'sadom', 'samckdaddy', 'sandm', 'sandnigger', 'satan', 'scag', 'scallywag', 'scat', 'schlong', 'screw', 'screwyou', 'scrotum', 'scum', 'semen ', 'seppo', 'servant', 'sex', 'sexed', 'sexfarm', 'sexhound', 'sexhouse', 'sexing', 'sexkitten', 'sexpot', 'sexslave', 'sextogo', 'sextoy', 'sextoys', 'sexual', 'sexually', 'sexwhore', 'sexy', 'sexymoma', 'sexy-slim', 'shag', 'shaggin', 'shagging', 'shat', 'shav', 'shawtypimp', 'sheeney', 'shhit', 'shinola', 'shit', 'shitcan', 'shitdick', 'shite', 'shiteater', 'shited', 'shitface', 'shitfaced', 'shitfit', 'shitforbrains', 'shitfuck', 'shitfucker', 'shitfull', 'shithapens', 'shithappens', 'shithead', 'shithouse', 'shiting', 'shitlist', 'shitola', 'shitoutofluck', 'shits', 'shitstain', 'shitted', 'shitter', 'shitting', 'shitty', 'shoot', 'shooting', 'shortfuck', 'showtime', 'sick', 'sissy', 'sixsixsix', 'sixtynine', 'sixtyniner', 'skank', 'skankbitch', 'skankfuck', 'skankwhore', 'skanky', 'skankybitch', 'skankywhore', 'skinflute', 'skum', 'skumbag', 'slant', 'slanteye', 'slapper', 'slaughter', 'slav', 'slave', 'slavedriver', 'sleezebag', 'sleezeball', 'slideitin', 'slime', 'slimeball', 'slimebucket', 'slopehead', 'slopey', 'slopy', 'slut', 'sluts', 'slutt', 'slutting', 'slutty', 'slutwear', 'slutwhore', 'smack', 'smackthemonkey', 'smut', 'snatch', 'snatchpatch', 'snigger', 'sniggered', 'sniggering', 'sniggers', \"snigger's\", 'sniper', 'snot', 'snowback', 'snownigger', 'sob ', 'sodom', 'sodomise', 'sodomite', 'sodomize', 'sodomy', 'sonofabitch', 'sonofbitch', 'sooty', 'sos ', 'soviet', 'spaghettibender', 'spaghettinigger', 'spank', 'spankthemonkey', 'sperm', 'spermacide', 'spermbag', 'spermhearder', 'spermherder', 'spic', 'spick', 'spig', 'spigotty', 'spik', 'spit', 'spitter', 'splittail', 'spooge', 'spreadeagle', 'spunk', 'spunky', 'squaw', 'stagg', 'stiffy', 'strapon', 'stringer', 'stripclub', 'stroke', 'stroking', 'stupid', 'stupidfuck', 'stupidfucker', 'suck', 'suckdick', 'sucker', 'suckme', 'suckmyass', 'suckmydick', 'suckmytit', 'suckoff', 'suicide', 'swallow', 'swallower', 'swalow', 'swastika', 'sweetness', 'syphilis', 'taboo', 'taff', 'tampon', 'tang', 'tantra', 'tarbaby', 'tard', 'teat', 'terror', 'terrorist', 'teste', 'testicle', 'testicles', 'thicklips', 'thirdeye', 'thirdleg', 'threesome', 'threeway', 'timbernigger', 'tinkle', 'titbitnipply', 'titfuck', 'titfucker', 'titfuckin', 'titjob', 'titlicker', 'titlover', 'tits', 'tittie', 'titties', 'titty', 'toilet', 'tongethruster', 'tongue', 'tonguethrust', 'tonguetramp', 'tortur', 'torture', 'tosser', 'towelhead', 'trailertrash', 'tramp', 'trannie', 'tranny', 'transexual', 'transsexual', 'transvestite', 'triplex', 'trisexual', 'trojan', 'trots', 'tuckahoe', 'tunneloflove', 'turd', 'turnon', 'twat', 'twink', 'twinkie', 'twobitwhore', 'uck ', 'unfuckable', 'upskirt', 'uptheass', 'upthebutt', 'urinary', 'urinate', 'urine', 'usama', 'uterus', 'vagina', 'vaginal', 'vatican', 'vibr', 'vibrater', 'vibrator', 'vietcong', 'violence', 'virgin', 'virginbreaker', 'vomit', 'vulva', 'wank', 'wanker', 'wanking', 'waysted', 'weapon', 'weenie', 'weewee', 'welcher', 'welfare', 'wetb', 'wetback', 'wetspot', 'whacker', 'whash', 'whigger', 'whiskey', 'whiskeydick', 'whiskydick', 'whitenigger', 'whites', 'whitetrash', 'whitey', 'whiz', 'whop', 'whore', 'whorefucker', 'whorehouse', 'wigger', 'willie', 'williewanker', 'willy', \"women's\", 'wuss', 'wuzzie', 'xxx', 'yankee', 'yellowman', 'zigabo', 'zipperhead'] # forbidden by Google\n",
    "\n",
    "def append_trash(trash_ngrams, trash_words_in_ngrams): # IDEA - short numbers are personal data and have no sence in covering\n",
    "    trash_nums = []\n",
    "    nums = []\n",
    "\n",
    "    for i in range(0, 10):\n",
    "        nums.append(str(i))\n",
    "        nums.append('0'+str(i))\n",
    "\n",
    "    for i in range(10, 32):\n",
    "        nums.append(str(i))\n",
    "\n",
    "    for num in nums:\n",
    "        trash_nums.append(' '+str(num))\n",
    "        trash_nums.append(str(num)+' ')\n",
    "\n",
    "    #print(trash_nums)\n",
    "\n",
    "    trash_words_in_ngrams += trash_nums\n",
    "    trash_ngrams += nums\n",
    "\n",
    "    for num in nums:\n",
    "        trash_ngrams.append(str(num)+',')\n",
    "        trash_ngrams.append(str(num)+'.')\n",
    "\n",
    "    print('trash_words_in_ngrams: ', trash_words_in_ngrams)\n",
    "    print('trash_ngrams: ', trash_ngrams)\n",
    "\n",
    "    return trash_ngrams, trash_words_in_ngrams\n",
    "\n",
    "def read_and_fix_df(path, x_test=False):\n",
    "    if path[-5:] == '.json':\n",
    "        df = pd.read_json(path)\n",
    "    else:\n",
    "        df = pd.read_csv(path)\n",
    "\n",
    "    if x_test: df = df.head(10000)\n",
    "\n",
    "    print(df['type'].value_counts())\n",
    "\n",
    "    if 'link_id' not in df.columns:\n",
    "        df['link_id'] = df['link'].apply(lambda x: re.findall(r\"https://my.wisepal.com/hapdne/mainapp/emailmessage/*([^/\\s\\\"]+)\", x)[0])\n",
    "\n",
    "    if 'context_type' not in df.columns:\n",
    "        df['context_type'] = 'email'\n",
    "\n",
    "    return df\n",
    "\n",
    "def extract_email_info(email):\n",
    "    subject_regex = r\"^(.*?)(?= from:)\"\n",
    "    from_section_regex = r\"from: (.*?)(?= to:)\"\n",
    "    email_regex = r\"([^\\s]+@[^\\s]+)\"\n",
    "    to_regex = r\"to: (?:.*? )?([^\\s]+@[^\\s]+)(?= \\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2})\"\n",
    "    date_time_regex = r\"(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2})\"\n",
    "    body_regex = r\"\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2} (.*)\"\n",
    "\n",
    "    subject = re.search(subject_regex, email, re.MULTILINE)\n",
    "    subject = subject.group(1).strip() if subject else None\n",
    "\n",
    "    from_section = re.search(from_section_regex, email, re.MULTILINE)\n",
    "    from_section = from_section.group(1).strip() if from_section else ''\n",
    "\n",
    "    from_email = re.search(email_regex, from_section)\n",
    "    from_email = from_email.group(1).strip() if from_email else None\n",
    "    from_email = from_email.replace('<', '').replace('>', '').replace(' ', '') if from_email else None\n",
    "\n",
    "    sender_name = None\n",
    "    if from_email:\n",
    "        sender_name = from_section.replace(from_email, '').strip()\n",
    "\n",
    "    to_section = re.search(to_regex, email, re.MULTILINE)\n",
    "    to_section = to_section.group(1).strip() if to_section else ''\n",
    "\n",
    "    to_email = re.search(email_regex, to_section)\n",
    "    to_email = to_email.group(1).strip() if to_email else None\n",
    "    to_email = to_email.replace('<', '').replace('>', '').replace(' ', '') if to_email else None\n",
    "\n",
    "    date_time = re.search(date_time_regex, email)\n",
    "    date_time = date_time.group(1).strip() if date_time else None\n",
    "    body = re.search(body_regex, email, re.MULTILINE | re.DOTALL)\n",
    "    body = body.group(1).strip() if body else None\n",
    "\n",
    "    return {'subject': subject, 'sender_name': sender_name, 'from_mail': from_email, 'to_email': to_email, 'date_time': date_time, 'body': body}\n",
    "\n",
    "def clean_text(text):\n",
    "    # features of Google API: any symbol except @, &, _, dot in number are supposed to be treated as space and 's too\n",
    "    text = text.replace('-', ' ')\n",
    "    text = re.sub(r'(?<!\\d)[.,](?!\\d)', ' ', text)\n",
    "    text = text.replace(\"'s\", ' ').replace(\"’s\", ' ')\n",
    "    text = re.sub(r'[^a-zA-Z0-9.,&]', ' ', text)\n",
    "\n",
    "    text = re.sub(r'\\s{2,}', ' ', text)\n",
    "    return text\n",
    "\n",
    "def generate_ngrams(text, n, n_grams):\n",
    "    email_entities = extract_email_info(text)\n",
    "    ngrams = []\n",
    "    if email_entities['subject']:\n",
    "        words_subject = clean_text(email_entities['subject']).split()\n",
    "        if n <= len(words_subject):\n",
    "            ngrams +=  [' '.join(words_subject[i:i + n]) for i in range(len(words_subject) - n + 1)]\n",
    "    if email_entities['sender_name']:\n",
    "        words_sender_name = clean_text(email_entities['sender_name']).split()\n",
    "        if n <= len(words_sender_name):\n",
    "            ngrams += [' '.join(words_sender_name[i:i + n]) for i in range(len(words_sender_name) - n + 1)]\n",
    "    if email_entities['from_mail']:\n",
    "        from_mail = clean_text(email_entities['from_mail']).split()\n",
    "        if n <= len(from_mail):\n",
    "            ngrams += [' '.join(from_mail[i:i + n]) for i in range(len(from_mail) - n + 1)]\n",
    "    if email_entities['body']:\n",
    "        words_body = clean_text(email_entities['body']).split()\n",
    "        if n <= len(words_body):\n",
    "            ngrams += [' '.join(words_body[i:i + n]) for i in range(len(words_body) - n + 1)]\n",
    "    if email_entities['from_mail'] and n == n_grams[0]:\n",
    "        ngrams += [email_entities['from_mail']]\n",
    "    if email_entities['to_email'] and n == n_grams[0]:\n",
    "        ngrams += [email_entities['to_email']]\n",
    "    return ngrams\n",
    "\n",
    "def generate_ngrams_pdf(text, n, n_grams):\n",
    "    ngrams = []\n",
    "    if text:\n",
    "        text = text.replace('\\n', ' ')\n",
    "        words_body = clean_text(text).split()\n",
    "        if n <= len(words_body):\n",
    "            ngrams += [' '.join(words_body[i:i + n]) for i in range(len(words_body) - n + 1)]\n",
    "    return ngrams\n",
    "\n",
    "def count_total_amount_clusters(df):\n",
    "    clusters_stats = df.groupby('type').agg(\n",
    "        count=('context', 'count')).sort_values('count', ascending=False)\n",
    "    total_amount_clusters = clusters_stats['count'].to_dict()\n",
    "    total_amount_clusters['good_types'] = len(df.loc[(df['type'] == 'good')].groupby(['link_id']).count())\n",
    "    total_amount_clusters['bad_types'] = len(df.loc[(df['type'] == 'bad')].groupby(['link_id']).count())\n",
    "    total_amount_clusters['neutral_types'] = len(df.loc[(df['type'] == 'neutral')].groupby(['link_id']).count())\n",
    "\n",
    "    return total_amount_clusters\n",
    "\n",
    "# algorithm Step 1: make 2 times for good and bad\n",
    "# make dict by ngrams: ngram, count in good/bad, set in good/bad\n",
    "# filter border good/bad, remove forbidden, remove alone letters - make (1)Good dict and (2)Bad dict\n",
    "def dict_by_clusters(df, total_amount_clusters, n_grams, good_types, bad_types, neutral_types,\n",
    "                     trash_ngrams, trash_words_in_ngrams, forbidden_words, ngrams_in_pdf,\n",
    "                     border_good_cover, border_bad_cover\n",
    "                     ):\n",
    "    df = df.copy() # save original df\n",
    "    df = df.sample(frac=1, random_state=0).reset_index(drop=True)\n",
    "\n",
    "    df['good_types'] = df['type'].isin(good_types)\n",
    "    df['bad_types'] = df['type'].isin(bad_types)\n",
    "    df['neutral_types'] = df['type'].isin(neutral_types)\n",
    "    cluster_unique = ['good_types_count', 'bad_types_count' , 'neutral_types_count'] #+ df['type'].unique().tolist()\n",
    "    clusters_count = defaultdict(lambda: {**{x: 0 for x in cluster_unique}, **{x: set() for x in ['good_set', 'bad_set', 'neutral_set']}})\n",
    "\n",
    "    progress_bar = tqdm(range(len(df)))\n",
    "    b1 = '\\033[1m'\n",
    "    b2 = '\\033[0m'\n",
    "\n",
    "    for i, row in df.iterrows(): # loop by emails\n",
    "        for n in range(n_grams[0], n_grams[1] + 1): # range by ngram size\n",
    "            text = row['context'] # email full text\n",
    "            type_label = row['type']  # email type\n",
    "            if row['context_type'] == 'email':\n",
    "                ngrams = generate_ngrams(text, n, n_grams) # make ngrams by email\n",
    "            else:\n",
    "                ngrams = generate_ngrams_pdf(text, n, n_grams)\n",
    "            # work with img-alt\n",
    "            if 'img_alts' in df.columns:\n",
    "                if row['img_alts']:\n",
    "                    img_alts = list(row['img_alts'].replace(\"'['\",\"\").replace(\"']'\",\"\").split(\"', '\"))\n",
    "                    for img_alt in img_alts:\n",
    "                        words_img_alt = clean_text(img_alt).split()\n",
    "                        if n <= len(words_img_alt):\n",
    "                            ngrams +=  [' '.join(words_img_alt[i:i + n]) for i in range(len(words_img_alt) - n + 1)]\n",
    "\n",
    "            #ind = row.name\n",
    "            ind = row['link_id']\n",
    "            for ngram in ngrams: # loop by ngrams\n",
    "                if ngram in trash_ngrams or any([x in ngram for x in trash_words_in_ngrams]): continue\n",
    "                if type_label in good_types:\n",
    "                    clusters_count[ngram]['good_types_count'] += 1 # word frequency: how many times the word appeared in good letters\n",
    "                    clusters_count[ngram]['good_set'].add(ind)\n",
    "                elif type_label in bad_types:\n",
    "                    clusters_count[ngram]['bad_types_count'] += 1\n",
    "                    clusters_count[ngram]['bad_set'].add(ind)\n",
    "                elif type_label in neutral_types:\n",
    "                    clusters_count[ngram]['neutral_types_count'] += 1\n",
    "                    clusters_count[ngram]['neutral_set'].add(ind)\n",
    "#                clusters_count[ngram][type_label] += 1\n",
    "\n",
    "        if (i > 0 and i % 10000 == 0) or (i == len(df) - 1):\n",
    "            #temp_dict.clear()\n",
    "            temp_dict = {ngram: data for ngram, data in clusters_count.items()\n",
    "                if len(clusters_count[ngram]['good_set']) / total_amount_clusters['good_types'] >= border_good_cover * (i / len(df))\n",
    "                or len(clusters_count[ngram]['bad_set']) / total_amount_clusters['bad_types'] >= border_bad_cover * (i / len(df))\n",
    "            }\n",
    "\n",
    "            print(f\"Iteration: {i}, clusters_count size: {len(clusters_count)}, temp_dict size: {len(temp_dict)}\")\n",
    "            clusters_count_size = sys.getsizeof(clusters_count)\n",
    "            for k, v in clusters_count.items():\n",
    "                for set_name in ['good_set', 'bad_set', 'neutral_set']:\n",
    "                    clusters_count_size += sys.getsizeof(v[set_name])\n",
    "\n",
    "            temp_dict_size = sys.getsizeof(temp_dict)\n",
    "            for k, v in temp_dict.items():\n",
    "                for set_name in ['good_set', 'bad_set', 'neutral_set']:\n",
    "                    temp_dict_size += sys.getsizeof(v[set_name])\n",
    "\n",
    "            print(f\"Size in RAM - clusters_count: {clusters_count_size / (2**30)} Gb, temp_dict: {temp_dict_size / (2**30)} Gb\")\n",
    "\n",
    "            for k, v in clusters_count.items():\n",
    "                if k not in temp_dict.keys():\n",
    "                    v['good_set'] = None\n",
    "                    v['bad_set'] = None\n",
    "                    v['neutral_set'] = None\n",
    "                    #gc.collect()\n",
    "                    v['good_set'] = set()\n",
    "                    v['bad_set'] = set()\n",
    "                    v['neutral_set'] = set()\n",
    "            clusters_count = None\n",
    "            #clusters_count.clear()\n",
    "            gc.collect()\n",
    "            clusters_count = defaultdict(lambda: {**{x: 0 for x in cluster_unique}, **{x: set() for x in ['good_set', 'bad_set', 'neutral_set']}}, temp_dict)\n",
    "\n",
    "#            with open(r'/content/drive/MyDrive/ssn_temp_dict v2.pkl', 'wb') as f:\n",
    "#                pickle.dump(temp_dict, f)\n",
    "\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    progress_bar = tqdm(range(len(clusters_count)))\n",
    "    for ngram in clusters_count:\n",
    "        clusters_count[ngram]['good_types_len'] = len(clusters_count[ngram]['good_set']) # покрытие слова: в каком количестве писем встречается слово\n",
    "        clusters_count[ngram]['bad_types_len'] = len(clusters_count[ngram]['bad_set']) # покрытие слова: в каком количестве писем встречается слово\n",
    "        clusters_count[ngram]['neutral_types_len'] = len(clusters_count[ngram]['neutral_set']) # покрытие слова: в каком количестве писем встречается слово\n",
    "#        clusters_count[ngram]['good_bad_len_dif'] = clusters_count[ngram]['good_types_len'] - clusters_count[ngram]['bad_types_len'] # полезность слова\n",
    "        clusters_count[ngram]['good_types_cover'] = clusters_count[ngram]['good_types_len'] / total_amount_clusters['good_types'] # покрытие слова %\n",
    "        clusters_count[ngram]['bad_types_cover'] = clusters_count[ngram]['bad_types_len'] / total_amount_clusters['bad_types'] # покрытие слова %\n",
    "#        clusters_count[ngram]['neutral_types_cover'] = clusters_count[ngram]['neutral_types_len'] / total_amount_clusters['neutral_types'] # покрытие слова %\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    calc_good = {ngram: data for ngram, data in clusters_count.items()\n",
    "      if data['good_types_cover'] >= border_good_cover\n",
    "        and len(ngram) > 2\n",
    "        and all([x not in f' {ngram} ' for x in forbidden_words])\n",
    "        }\n",
    "    # list of -words\n",
    "    calc_bad = {ngram: data for ngram, data in clusters_count.items()\n",
    "      if data['bad_types_cover'] >= border_bad_cover\n",
    "        and data['good_types_len'] == 0\n",
    "        and data['neutral_types_len'] == 0\n",
    "        and len(ngram) > 2\n",
    "        and all([x not in f' {ngram} ' for x in forbidden_words])\n",
    "        and all([x not in f' {ngram} ' for x in ngrams_in_pdf])\n",
    "        }\n",
    "\n",
    "    for k, v in clusters_count.items():\n",
    "        if k not in calc_good.keys():\n",
    "            if k not in calc_bad.keys():\n",
    "                v['good_set'] = None\n",
    "                v['bad_set'] = None\n",
    "                v['neutral_set'] = None\n",
    "    clusters_count = None\n",
    "    #clusters_count.clear()\n",
    "    gc.collect()\n",
    "\n",
    "    return calc_good, calc_bad\n",
    "\n",
    "def user_unreached(df, union_set, cur_set):\n",
    "    # list of unreached user/company pairs\n",
    "    if 'company' in df.columns and 'importance' in df.columns:\n",
    "        user = df.loc[df['link_id'].isin(union_set)].groupby(['user_id', 'company']).count()\n",
    "        user_dict = user['importance'].to_dict()\n",
    "\n",
    "        user_reached = df.loc[df['link_id'].isin(cur_set)].groupby(['user_id', 'company']).count()\n",
    "        user_r_dict = user_reached['importance'].to_dict()\n",
    "    else:\n",
    "        user = df.loc[df['link_id'].isin(union_set)].groupby(['user_id']).count()\n",
    "        user_dict = user['context'].to_dict()\n",
    "\n",
    "        user_reached = df.loc[df['link_id'].isin(cur_set)].groupby(['user_id']).count()\n",
    "        user_r_dict = user_reached['context'].to_dict()\n",
    "\n",
    "    for k in user_r_dict.keys():\n",
    "        del user_dict[k]\n",
    "\n",
    "    return user_dict\n",
    "\n",
    "#02.04.2024+07.04.2024\n",
    "# hypothesis - the weight of an email is inversely proportional to the average coverage of its included ngrams\n",
    "# IDEA TF-IDF like - we first strive to select ngrams that cover as many rare emails as possible on average\n",
    "def email_ranking(df, calc, set_name='good_set'):\n",
    "    calc = calc.copy()\n",
    "    timer = defaultdict(int)\n",
    "\n",
    "    union_set = set.union(*(value[set_name] for value in calc.values()))\n",
    "\n",
    "    email_rank = defaultdict(list)\n",
    "    tic = time.time()\n",
    "    for ngram_i, value_i in calc.items():\n",
    "        for email in value_i[set_name]:\n",
    "            # for each email I change its ngrams to the quantitative coverage of this ngram\n",
    "            email_rank[email].append(len(value_i[set_name]))\n",
    "    timer['email_rank'] += time.time()-tic\n",
    "\n",
    "    # recalculate the inverse of the mean - if the in mean the email has rare ngrams = higher score\n",
    "    if 'importance' in df.columns:\n",
    "        email_rank = {k: np.mean(v) for k, v in email_rank.items()}\n",
    "        # normalization before multiplication\n",
    "        min_rank = min(email_rank.values())\n",
    "        max_rank = max(email_rank.values())\n",
    "        min_imp = df.loc[((df['type'] == 'good'))]['importance'].min()\n",
    "        max_imp = df.loc[((df['type'] == 'good'))]['importance'].max()\n",
    "        email_rank = {k: 1 - (((v-min_rank)/(max_rank-min_rank)) * ((df.loc[(df.index==k)]['importance'].values[0]-min_imp)/(max_imp-min_imp)))\n",
    "                             for k, v in email_rank.items()}\n",
    "    else:\n",
    "        email_rank = {k: np.mean(v) for k, v in email_rank.items()}\n",
    "        min_rank = min(email_rank.values())\n",
    "        max_rank = max(email_rank.values())\n",
    "        email_rank = {k: 1 - ((v-min_rank)/(max_rank-min_rank)) for k, v in email_rank.items()}\n",
    "\n",
    "    return email_rank\n",
    "\n",
    "# 24.03.2024 minus words\n",
    "def dict_minus_ngram(df, calc_bad, total_amount_clusters, total_bad_set, max_len=500, min_diff=0.005, x_email_rank=False, email_only=False):\n",
    "    calc_bad = calc_bad.copy()\n",
    "    ### idea - only bad email adresses and domains - for more safety\n",
    "    if email_only:\n",
    "        calc_bad = {ngram: data for ngram, data in calc_bad.items()\n",
    "        if '@' in ngram or ngram[-4:] == ' com'\n",
    "        }\n",
    "    # only minus words to the remaining bad coverage\n",
    "    for ngram, data in calc_bad.items():\n",
    "        data['bad_types_cover'] = len(data['bad_set'] & total_bad_set) / total_amount_clusters['bad_types'] # word cover %\n",
    "\n",
    "    calc_bad = {ngram: data for ngram, data in calc_bad.items()\n",
    "      if len(data['bad_set'] & total_bad_set) > 0\n",
    "      and len(data['good_set']) == 0\n",
    "    }\n",
    "\n",
    "    calc_bad = {k: v for k, v in sorted(calc_bad.items(), key=lambda x: (x[1]['bad_types_cover'], -len(x[0])), reverse=True)}\n",
    "\n",
    "    all_ngrams = list(calc_bad.keys())\n",
    "    all_ngrams.pop(0)\n",
    "\n",
    "    cur_diff = 0\n",
    "    curr = {\n",
    "        'ngram_list': [next(iter(calc_bad))],\n",
    "        'bad_set': (set() | set(calc_bad[next(iter(calc_bad))]['bad_set'])),\n",
    "        }\n",
    "    print(f\"{curr['ngram_list']}\\n Bad: {round(len(curr['bad_set']) / len(total_bad_set) * 100, 1)}%; Dif. in bads: {round(len(curr['bad_set']) / len(total_bad_set) * 100, 1)}%; Length: {len('   '.join(curr['ngram_list']))}\")\n",
    "\n",
    "    if x_email_rank: email_rank_bad = email_ranking(df, calc_bad, set_name='bad_set')\n",
    "\n",
    "    progress_bar = tqdm(range(0, len(total_bad_set)))\n",
    "    progress_bar.update(len(curr['bad_set']))\n",
    "    while len(all_ngrams) > 0:\n",
    "        curr_bad_perc = len(curr['ngram_list']) / len(total_bad_set)\n",
    "        cur_diff = 0\n",
    "        if x_email_rank: progress_bar_ngram = tqdm(range(0, len(all_ngrams)))\n",
    "        for new_ngram, new_data in calc_bad.items():\n",
    "            if x_email_rank: progress_bar_ngram.update(1)\n",
    "            if new_ngram in all_ngrams:\n",
    "                if not x_email_rank:\n",
    "                    new_diff = len((new_data['bad_set'] - curr['bad_set']) & total_bad_set) / len(total_bad_set)\n",
    "                else:\n",
    "                    new_diff = sum([v for k,v in email_rank_bad.items() if k in (new_data['bad_set'] - curr['bad_set']) & total_bad_set])\n",
    "                if new_diff > cur_diff:\n",
    "                    curr_ngram = new_ngram\n",
    "                    cur_diff = new_diff\n",
    "                if cur_diff <= min_diff:\n",
    "                    all_ngrams.remove(new_ngram)\n",
    "\n",
    "        if cur_diff <= min_diff:\n",
    "            break\n",
    "        if sum([len(x) for x in curr['ngram_list']]) + len(curr_ngram) > max_len:\n",
    "            break\n",
    "\n",
    "        progress_bad_cov = len((calc_bad[curr_ngram]['bad_set'] - curr['bad_set']) & total_bad_set)\n",
    "        progress_bar.update(progress_bad_cov)\n",
    "\n",
    "        curr['bad_set'] |= calc_bad[curr_ngram]['bad_set']\n",
    "        curr['ngram_list'].append(curr_ngram)\n",
    "        all_ngrams.remove(curr_ngram)\n",
    "\n",
    "        print(f\"{curr['ngram_list']}\\n Bad: {round(len(curr['bad_set']) / len(total_bad_set) * 100, 1)}%; Dif. in bads: {round(cur_diff * 100, 1)}%; Length: {sum([len(x) for x in curr['ngram_list']])}\\n\")\n",
    "\n",
    "    return curr\n",
    "\n",
    "x_print = True  # debug print if True\n",
    "\n",
    "def dict_group_ngrams_withminus(df, calc_good, total_amount_clusters, ngrams_init=[[]], ngrams_last=[], req_good_set=set(), minus_bad_set=set(), try_max=8, max_len=1500,\n",
    "                                target_good_cover=0.95, cut_bad_cover=0.5, step_bad_cover=0.1, target_good_inc=2, user_comp_imp=0, x_user_comp=False, x_tfidf=True):\n",
    "    timer = defaultdict(int)\n",
    "    calc_good = calc_good.copy()\n",
    "\n",
    "    groups_good = {}\n",
    "\n",
    "    union_good_set = set.union(*(value['good_set'] for value in calc_good.values())) # achievable goods\n",
    "    union_bad_set = set.union(*(value['bad_set'] for value in calc_good.values())) - minus_bad_set # achievable bads\n",
    "\n",
    "    progress_bar = tqdm(range(0, len(union_bad_set)))\n",
    "\n",
    "    i = 0 # group number\n",
    "\n",
    "    total_len = 0\n",
    "    try_num = 0\n",
    "    curr_cut_bad_cover = cut_bad_cover\n",
    "    final_cover_good = 1\n",
    "    forb_ngrams = set()\n",
    "\n",
    "    email_rank_good = email_ranking(df, calc_good, set_name='good_set')\n",
    "#    email_rank_bad = email_ranking(calc_good, set_name='bad_set')\n",
    "    user_comb = len(user_unreached(df, union_good_set, set()))\n",
    "    df_dict = df[['user_id']].to_dict('index')\n",
    "\n",
    "    if ngrams_init != [[]]: # the ability to start collecting not from the first group\n",
    "        for ngrams in ngrams_init:\n",
    "            curr = {\n",
    "                'ngram_list': [],\n",
    "                'good_set': set(),\n",
    "                'bad_set': set(),\n",
    "            }\n",
    "            for ngram in ngrams:\n",
    "                curr['ngram_list'].append(ngram)\n",
    "                curr['good_set'] |= calc_good[ngram]['good_set']\n",
    "                curr['bad_set'] |= calc_good[ngram]['bad_set']\n",
    "            group_len = sum([len(x) for x in curr['ngram_list']])\n",
    "\n",
    "            total_len += group_len\n",
    "            progress_bad_cov = len(union_bad_set - curr['bad_set'])\n",
    "            progress_bar.update(progress_bad_cov)\n",
    "            union_bad_set &= curr['bad_set']\n",
    "            groups_good[i] = curr.copy()\n",
    "            i += 1\n",
    "\n",
    "    while True: # group building loop\n",
    "        if x_print: print('Iteration №', try_num)\n",
    "        try_num += 1\n",
    "        curr = {\n",
    "            'ngram_list': [],\n",
    "            'good_set': set(),\n",
    "            'bad_set': set(),\n",
    "        }\n",
    "        cover_prev = {'good': 0, 'bad': 0, 'inter': 0}\n",
    "        curr_req_good_set = set()\n",
    "        curr_req_good_set |= req_good_set # First cover the required part using only ngrams from required emails\n",
    "        group_len = 0\n",
    "        forb_ngrams = set()\n",
    "\n",
    "        if ngrams_last != []:\n",
    "            for ngram in ngrams_last:\n",
    "                curr['ngram_list'].append(ngram)\n",
    "                curr['good_set'] |= calc_good[ngram]['good_set']\n",
    "                curr['bad_set'] |= calc_good[ngram]['bad_set']\n",
    "            group_len = sum([len(x) for x in curr['ngram_list']])\n",
    "            curr_req_good_set -= curr['good_set']\n",
    "            ngrams_last = []\n",
    "\n",
    "        while True:\n",
    "            if i == 0:\n",
    "                total_good_set = union_good_set\n",
    "            else:\n",
    "                total_good_set = set.intersection(*(value['good_set'] for value in groups_good.values())) # do not take into account the coverage+ that will be lost after crossing groups\n",
    "            score_dict = {}\n",
    "            tici = time.time() ## time reference point\n",
    "            len_union_bad_set = len(union_bad_set)\n",
    "            if user_comp_imp > 0:\n",
    "                tic = time.time()\n",
    "                user_dict = user_unreached(df, union_good_set, curr['good_set'] & total_good_set) # not yet covered user/company matchings # INTERSECT WITH PREVIOUS GROUP\n",
    "                timer['user_dict'] += time.time()-tic\n",
    "#                if x_user_comp and len(user_company_dict) == 0: # for a narrow query covering only user/company matchings\n",
    "#                    break\n",
    "            progress_bar_ngram = tqdm(range(0, len(calc_good)))\n",
    "            bad_acc = (len(curr['good_set'] & total_good_set)/len(total_good_set)) - ((1/(1 - curr_cut_bad_cover)) * (len(curr['bad_set'] & union_bad_set) / len(union_bad_set))) # what percentage of coverage- was previously saved relative to the percentage of coverage+\n",
    "            #print(bad_acc)\n",
    "            for ngram_j, value_j in calc_good.items():\n",
    "                progress_bar_ngram.update(1)\n",
    "                tic = time.time()\n",
    "                if ngram_j in forb_ngrams: ### IDEA - time saving on very bad ones\n",
    "                    timer['forb_ngrams'] += time.time()-tic\n",
    "                    continue\n",
    "                timer['forb_ngrams'] += time.time()-tic\n",
    "                tic = time.time()\n",
    "                good_set_diff = (value_j['good_set'] - curr['good_set']) & total_good_set # how much coverage+ will be added by the ngram # INTERSECT WITH PREVIOUS GROUP\n",
    "                timer['good_set_diff'] += time.time()-tic\n",
    "                if len(good_set_diff) >= max(1, target_good_inc-i): # new ngram must add more than 1 email to coverage+\n",
    "                    if curr_req_good_set:\n",
    "                        if len(good_set_diff & curr_req_good_set) == 0: # first cover the required set using only ngrams from required emails\n",
    "                            continue\n",
    "                    # ngram should do more good than harm\n",
    "                    tic = time.time()\n",
    "                    bad_set_diff = (value_j['bad_set'] - curr['bad_set']) & union_bad_set\n",
    "                    benifit_good = len(good_set_diff)/len(total_good_set)\n",
    "                    benifit_bad = (1/(1 - curr_cut_bad_cover)) * len(bad_set_diff) / len(union_bad_set)\n",
    "                    timer['bad_set_diff_1'] += time.time()-tic\n",
    "                    #if len(good_set_diff)/len(total_good_set) < (1/(1 - curr_cut_bad_cover)) * len(bad_set_diff) / len(union_bad_set) - bad_acc:\n",
    "                    if benifit_good < benifit_bad - bad_acc:\n",
    "                        continue\n",
    "                    tic = time.time()\n",
    "                    ## Takes a long time\n",
    "                    combined_bad_set = curr['bad_set'] | value_j['bad_set'] # bed set that will be on the next iteration\n",
    "                    timer['combined_bad_set'] += time.time()-tic\n",
    "                    tic = time.time()\n",
    "                    bad_set_diff = union_bad_set - combined_bad_set\n",
    "                    timer['bad_set_diff'] += time.time()-tic\n",
    "                    ## Takes a long time\n",
    "                    if len(bad_set_diff) > len_union_bad_set * curr_cut_bad_cover:\n",
    "#                        if len(curr['ngram_list']) < target_group_len:\n",
    "                            tic = time.time()\n",
    "                            if x_tfidf:\n",
    "                                score_dict[ngram_j] = sum([v for k,v in email_rank_good.items() if k in good_set_diff]) # bonus for uniqueness and length of coverage+\n",
    "                            else:\n",
    "                                score_dict[ngram_j] = len(good_set_diff)\n",
    "                            timer['score_dict_good'] += time.time()-tic\n",
    "                            # the query should to return one letter for each user/company pair\n",
    "                            # list of unreached user/company pairs\n",
    "                            if user_comp_imp > 0:\n",
    "                                tic = time.time()\n",
    "                                if user_dict:\n",
    "                                    user_dict_copy = user_dict.copy()\n",
    "                                    df_dict_filtered = {k: v for k,v in df_dict.items() if k in good_set_diff}\n",
    "                                    # bonus only for first entry...\n",
    "                                    for row in df_dict_filtered.values():\n",
    "                                        user_key = row['user_id']\n",
    "                                        if user_key in user_dict_copy:\n",
    "                                            score_dict[ngram_j] = score_dict[ngram_j] + user_comp_imp * (1 + 0.5*i) # 57 = 1% гуд сета, потом полтора ### проверить на выбросы\n",
    "                                            del user_dict_copy[user_key]\n",
    "                                timer['user_score'] += time.time()-tic\n",
    "                    else: ### IDEA - time saving on very bad ones\n",
    "                        tic = time.time()\n",
    "                        forb_ngrams.add(ngram_j)\n",
    "                        timer['forb_ngrams'] += time.time()-tic\n",
    "                else: ### IDEA - time saving on very bad ones\n",
    "                    tic = time.time()\n",
    "                    forb_ngrams.add(ngram_j)\n",
    "                    timer['forb_ngrams'] += time.time()-tic\n",
    "\n",
    "            if not score_dict: # break the loop if there are no ngrams left adding coverage+\n",
    "                break\n",
    "            # add to group\n",
    "            tic = time.time()\n",
    "            best_ngram = max(score_dict, key=score_dict.get)\n",
    "            ### idea - if ngrams are equal, choose an ngram that covers more of the good ones - maximum overlap for reliability of hitting the query\n",
    "            best_dict = {k: len(calc_good[k]['good_set']) for k,v in score_dict.items() if v == score_dict[best_ngram]} # len(calc_good[k]['good_set'])/total_amount_clusters['good_types'] - len(calc_good[k]['bad_set'])/total_amount_clusters['bad_types']\n",
    "            #print(best_dict, ' len forb_ngrams', len(forb_ngrams)) #({k:v for k,v in score_dict.items() if v == score_dict[best_ngram]})\n",
    "            best_ngram = max(best_dict, key=best_dict.get)\n",
    "            ###\n",
    "            timer['best_ngram'] += time.time()-tic\n",
    "            if x_print: print('time of iter:', round(time.time()-tici, 2), 'len of score_dict', len(score_dict),\n",
    "#                              'filtered', len(score_dict_filtered),\n",
    "                              'added good', calc_good[best_ngram]['good_set'] - curr['good_set'])\n",
    "\n",
    "            if total_len+group_len+sum([len(x) for x in best_ngram]) > max_len: # break the loop if too long group\n",
    "                break\n",
    "\n",
    "            for ngram_big in curr['ngram_list']: # removal of ngrams that are part of more general ngrams\n",
    "                if best_ngram+' ' in ngram_big or ' '+best_ngram in ngram_big:\n",
    "                    if x_print: print('ngram \"', ngram_big, '\" was removed by \"', best_ngram, '\"')\n",
    "                    curr['ngram_list'].remove(ngram_big)\n",
    "\n",
    "\n",
    "            curr['ngram_list'].append(best_ngram)\n",
    "            curr['good_set'] |= calc_good[best_ngram]['good_set']\n",
    "            curr['bad_set'] |= calc_good[best_ngram]['bad_set']\n",
    "            group_len = sum([len(x) for x in curr['ngram_list']])\n",
    "\n",
    "            if curr_req_good_set:\n",
    "                curr_req_good_set -= curr['good_set']\n",
    "                if len(curr_req_good_set) == 0:\n",
    "                    print('--- Requied good set was cowered ---')\n",
    "\n",
    "            if x_print:\n",
    "                print('group №:', i, 'target_cut:', curr_cut_bad_cover, 'len:', group_len, '(', total_len+group_len, ')',\n",
    "                              'cover good:', round(len(curr['good_set'])/total_amount_clusters['good_types'], 4), '(+', round(len(curr['good_set'])/total_amount_clusters['good_types']-cover_prev['good'], 4), ')',\n",
    "                              'cover bad:', round(len(curr['bad_set'])/total_amount_clusters['bad_types'], 4), '(+', round(len(curr['bad_set'])/total_amount_clusters['bad_types']-cover_prev['bad'], 4), ')',\n",
    "                              'cover bad inter:', round(len(union_bad_set & curr['bad_set'])/len_union_bad_set, 4), '(+', round(len(union_bad_set & curr['bad_set'])/len_union_bad_set-cover_prev['inter'], 4), ')',\n",
    "                              'unreached combination len:', len(user_unreached(df, union_good_set, curr['good_set'] & total_good_set)), '//', user_comb, 'unreached requied:', len(curr_req_good_set), '//', len(req_good_set),  ### ПЕРЕСЕЧЬ С ПРЕДЫДУЩЕЙ ГРУППОЙ???\n",
    "                              '\\n', len(curr['ngram_list']), curr['ngram_list'])\n",
    "                cover_prev['good'] = round(len(curr['good_set'])/total_amount_clusters['good_types'], 4)\n",
    "                cover_prev['bad'] = round(len(curr['bad_set'])/total_amount_clusters['bad_types'], 4)\n",
    "                cover_prev['inter'] = round(len(union_bad_set & curr['bad_set'])/len_union_bad_set, 4)\n",
    "            if x_print: print(timer.items())\n",
    "\n",
    "        if x_print: print(timer.items())\n",
    "        if len(set.intersection(*(value['good_set'] for value in groups_good.values()), curr['good_set'])) / total_amount_clusters['good_types'] > target_good_cover:\n",
    "            if total_len+group_len <= max_len: # only groups with the target length are saved\n",
    "                progress_bad_cov = len(union_bad_set - curr['bad_set'])\n",
    "                progress_bar.update(progress_bad_cov)\n",
    "                union_bad_set &= curr['bad_set']\n",
    "                if not progress_bad_cov: # break the loop if the new group does not cut anything off from the general coverage-\n",
    "                    break\n",
    "\n",
    "                total_len += group_len\n",
    "                groups_good[i] = curr.copy()\n",
    "                curr_cut_bad_cover = cut_bad_cover\n",
    "                final_cover_good = len(set.intersection(*(value['good_set'] for value in groups_good.values()))) / total_amount_clusters['good_types']\n",
    "                if x_print: print('+++Continue. Cover bad by groups:', round(len(union_bad_set) / total_amount_clusters['bad_types'], 4), 'Cover good by groups:', round(final_cover_good, 4), 'Groups length:', sum([sum([len(y) for y in x['ngram_list']]) for x in groups_good.values()]), '\\n')\n",
    "                if not len(union_bad_set): # break the loop if the coverage- is over\n",
    "                    break\n",
    "                #for ngram in curr['ngram_list']: all_ngrams.append(ngram)\n",
    "\n",
    "                i += 1 # a new group\n",
    "            else:\n",
    "                if x_print: print('Over len. Cover bad by groups:', round(len(union_bad_set & curr['bad_set']) / total_amount_clusters['bad_types'], 4), 'Cover good by groups:', round(final_cover_good*len(curr['good_set'])/total_amount_clusters['good_types'], 4), 'Groups length:', total_len+group_len, '\\n')\n",
    "                curr_cut_bad_cover -= step_bad_cover\n",
    "        else:\n",
    "            if x_print: print('Not enouth. Over Cover bad by groups:', round(len(union_bad_set & curr['bad_set']) / total_amount_clusters['bad_types'], 4), 'Cover good by groups:', round(final_cover_good*len(curr['good_set'])/total_amount_clusters['good_types'], 4), 'Groups length:', total_len+group_len, '\\n')\n",
    "            curr_cut_bad_cover -= step_bad_cover\n",
    "\n",
    "        if curr_cut_bad_cover < step_bad_cover: # break the loop if the new group does not cut off anything from the overall coverage-\n",
    "            break\n",
    "\n",
    "        if try_num > try_max:\n",
    "            break\n",
    "\n",
    "        if group_len > max_len * 0.9:\n",
    "            break\n",
    "\n",
    "    for group in groups_good.values():\n",
    "        group['good_types_cover'] = len(group['good_set']) / total_amount_clusters['good_types'] # final coverage+ %\n",
    "        group['bad_types_cover'] = len(group['bad_set']) / total_amount_clusters['bad_types'] # final coverage- %\n",
    "\n",
    "    if x_print: print(total_len, timer.items())\n",
    "\n",
    "    return groups_good, total_len, final_cover_good, len(union_bad_set) / total_amount_clusters['bad_types']\n",
    "\n",
    "def groups_to_query(groups_good, base_query, total_amount_clusters):\n",
    "    total_good_set = set()\n",
    "    total_bad_set = set()\n",
    "    query = base_query + ' '\n",
    "\n",
    "    total_good_set |= groups_good[0]['good_set']\n",
    "    total_bad_set |= groups_good[0]['bad_set']\n",
    "    for i in range(0, int(len(groups_good))):\n",
    "        total_good_set &= groups_good[i]['good_set']\n",
    "        total_bad_set &= groups_good[i]['bad_set']\n",
    "        query = query + '('\n",
    "        for ngram in groups_good[i]['ngram_list']:\n",
    "            if ' ' in ngram:\n",
    "                query = query + '\\\"' + ngram + '\\\"|'\n",
    "            else:\n",
    "                query = query + ngram + '|'\n",
    "        query = query[:-1] + ') '\n",
    "        print(f\"Total cover of {i} groups: len {len(query)}, good {len(total_good_set) / total_amount_clusters['good_types']}, bad {len(total_bad_set) / total_amount_clusters['bad_types']}\")\n",
    "    query = query[:-1]\n",
    "    query = query + ' -wisepal'\n",
    "\n",
    "    return query, total_good_set, total_bad_set\n",
    "\n",
    "def minus_to_query(group_minus, base_query):\n",
    "    query = base_query + ' -'\n",
    "    for ngram in group_minus['ngram_list']:\n",
    "        if ' ' in ngram:\n",
    "            query = query + '\\\"' + ngram + '\\\" '\n",
    "        else:\n",
    "            query = query + ngram + ' '\n",
    "        query = query[:-1]+' -'\n",
    "\n",
    "    query = query[:-2]\n",
    "\n",
    "    return query\n",
    "\n",
    "# final union function\n",
    "def pipeline_of_query_creating(path_params, ngram_params, minus_params, group_params, base_query, x_test):\n",
    "    ### fix df ###\n",
    "    print('### Reading df ###')\n",
    "    df = read_and_fix_df(path_params['df'], x_test=x_test)\n",
    "\n",
    "    ### Generate ngram dict ###\n",
    "    print('### Generate ngram dict ###')\n",
    "\n",
    "    total_amount_clusters = count_total_amount_clusters(df)\n",
    "    print('border_good:', int(total_amount_clusters['good_types'] * ngram_params['border_good_cover'] + 1), '/', total_amount_clusters['good_types'])\n",
    "    print(' border_bad:', int(total_amount_clusters['bad_types'] * ngram_params['border_bad_cover'] + 1), '/', total_amount_clusters['bad_types'])\n",
    "\n",
    "    calc_good = calc_bad = {}\n",
    "    if path_params['wb_calc'] == False:\n",
    "        try:\n",
    "            with open(path_params['calc_good'], 'rb') as f:\n",
    "                calc_good = pickle.load(f)\n",
    "\n",
    "            with open(path_params['calc_bad'], 'rb') as f:\n",
    "                calc_bad = pickle.load(f)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    if len(calc_good) == 0:\n",
    "        gc.collect()\n",
    "        tic = time.time()\n",
    "        calc_good, calc_bad = dict_by_clusters(df, total_amount_clusters, n_grams=ngram_params['n_grams'],\n",
    "                                               good_types=ngram_params['good_types'], bad_types=ngram_params['bad_types'], neutral_types=ngram_params['neutral_types'],\n",
    "                                               trash_ngrams=ngram_params['trash_ngrams'], trash_words_in_ngrams=ngram_params['trash_words_in_ngrams'],\n",
    "                                               forbidden_words=ngram_params['forbidden_words'], ngrams_in_pdf=ngram_params['ngrams_in_pdf'],\n",
    "                                               border_good_cover=ngram_params['border_good_cover'], border_bad_cover=ngram_params['border_bad_cover'])\n",
    "\n",
    "        print(f'###### Time of dict_by_clusters ', time.time()-tic, ' Len of calc_good ', len(calc_good), ' ###\\n')\n",
    "        gc.collect()\n",
    "\n",
    "        with open(path_params['calc_good'], 'wb') as f:\n",
    "            pickle.dump(calc_good, f)\n",
    "\n",
    "        with open(path_params['calc_bad'], 'wb') as f:\n",
    "            pickle.dump(calc_bad, f)\n",
    "\n",
    "    ### Generate minus words ###\n",
    "    print('### Generate minus words ###')\n",
    "    group_minus = {}\n",
    "    if path_params['wb_group_minus'] == False:\n",
    "        try:\n",
    "            with open(path_params['group_minus'], 'rb') as f:\n",
    "                groups = pickle.load(f)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    if len(group_minus) == 0:\n",
    "        tic = time.time()\n",
    "        group_minus = dict_minus_ngram(df, calc_bad, total_amount_clusters, set.union(*(value['bad_set'] for value in calc_good.values())),\n",
    "                                    max_len=minus_params['max_len'], min_diff=minus_params['min_diff'], x_email_rank=minus_params['x_email_rank'], email_only=minus_params['email_only'])\n",
    "        rest_minus_len = minus_params['max_len'] - sum([len(x) for x in group_minus['ngram_list']])\n",
    "        print(f'###### Time of dict_minus_ngram ', time.time()-tic, ' Rest len of group_minus ', rest_minus_len, ' ###\\n')\n",
    "\n",
    "        with open(path_params['group_minus'], 'wb') as f:\n",
    "            pickle.dump(group_minus, f)\n",
    "\n",
    "    ### Generate ngram groups ###\n",
    "    print('### Generate ngram groups ###')\n",
    "    groups = {}\n",
    "    if path_params['wb_groups'] == False:\n",
    "        try:\n",
    "            with open(path_params['groups'], 'rb') as f:\n",
    "                groups = pickle.load(f)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    if len(groups) == 0:\n",
    "        tic = time.time()\n",
    "        groups, total_len, good_cover, bad_cover = dict_group_ngrams_withminus(df, calc_good, total_amount_clusters,\n",
    "                                                                               ngrams_init=group_params['ngrams_init'], ngrams_last=group_params['ngrams_last'],\n",
    "                                                                               req_good_set=group_params['req_good_set'], minus_bad_set=group_minus['bad_set'],\n",
    "                                                                               try_max=group_params['try_max'], max_len=group_params['max_len']+rest_minus_len,\n",
    "                                                                               target_good_cover=group_params['target_good_cover'], cut_bad_cover=group_params['cut_bad_cover'], step_bad_cover=group_params['step_bad_cover'],\n",
    "                                                                               target_good_inc=group_params['target_good_inc'], user_comp_imp=group_params['user_comp_imp'],\n",
    "                                                                               x_user_comp=group_params['x_user_comp'], x_tfidf=group_params['x_tfidf'])\n",
    "        print(f'###### Time of dict_group_ngrams_withminus ', time.time()-tic, ' ###\\n')\n",
    "        with open(path_params['groups'], 'wb') as f:\n",
    "            pickle.dump(groups, f)\n",
    "\n",
    "    ### Concatinate groups to query ###\n",
    "    print('### Concatinate groups to query ###')\n",
    "    query, total_good_set, total_bad_set = groups_to_query(groups, base_query, total_amount_clusters)\n",
    "    print('Query with groups len: ', len(query))\n",
    "    #print(query)\n",
    "\n",
    "    query = minus_to_query(group_minus, query)\n",
    "    print(\"Final bad cover:\", len(total_bad_set - group_minus['bad_set']) / total_amount_clusters['bad_types'])\n",
    "    print(f'###### Final query len: ', len(query), ' ###\\n')\n",
    "    print(query)\n",
    "\n",
    "    ### Outliers ###\n",
    "    print(f'\\n### Outliers ###')\n",
    "    union_good_set = set.union(*(value['good_set'] for value in calc_good.values()))\n",
    "    out_good_set = union_good_set - total_good_set\n",
    "    print('Total good set len:', len(union_good_set), ' Outliers good set len:', len(out_good_set), ' outliers:', out_good_set)\n",
    "\n",
    "    pd.options.display.max_colwidth = 250\n",
    "    for link_id in out_good_set:\n",
    "        print(df.loc[df['link_id'] == link_id, ['user_id', 'context_type', 'link']].to_dict('records'))\n",
    "\n",
    "    return df, calc_good, calc_bad, group_minus, groups, query, out_good_set\n",
    "\n",
    "# input data\n",
    "path_params = {'df': r'/Users/shahardubiner/Documents/WisePal/ssn_df_grouped_fixman v3.json',\n",
    "                'wb_calc': True,  # rewrite\n",
    "                'calc_good': r'ssn_test_calc_good_v3.pkl',\n",
    "                'calc_bad': r'ssn_test_calc_bad_v3.pkl',\n",
    "                'wb_group_minus': True,  # rewrite\n",
    "                'group_minus': r'ssn_test_group_minus_v3.pkl',\n",
    "                'wb_groups': True,  # rewrite\n",
    "                'groups': r'ssn_test_groups_v3.pkl'\n",
    "}\n",
    "\n",
    "\n",
    "ngram_params = {'n_grams': [1,4],\n",
    "                'good_types': ['good'], # class names\n",
    "                'bad_types': ['bad'], # class names\n",
    "                'neutral_types': ['neutral'],\n",
    "                'forbidden_words': forbidden_words, # forbidden by Google\n",
    "                'trash_ngrams': ['45am', '3.2', '2.3', '2013,', '01,', '26,', '43pm', '26th', '1967,', '45pm', '22nd', '35pm', '2.75', '59pm', '59am', '00pm', '00am'], # remove odd\n",
    "                'trash_words_in_ngrams': ['gmail'], # remove user's email adresses\n",
    "                'ngrams_in_pdf': [\"twitter com\", \"greg hdcomptoninsurance com\"], # forbidden for use like minus word, becouse they included in img-alt, href or pdf\n",
    "                'border_good_cover': 0.001, # 0.0004, # down border\n",
    "                'border_bad_cover': 0.003 # 0.001 # down border\n",
    "}\n",
    "\n",
    "minus_params = {'max_len': 500,\n",
    "                'min_diff': 0.001,\n",
    "                'x_email_rank': False,\n",
    "                'email_only': True # safty mode when minus words are emeil adresses and domains only\n",
    "                }\n",
    "\n",
    "group_params = {'ngrams_init': [[]], # if need to start from second or third group\n",
    "                'ngrams_last': [], # if need to continue group\n",
    "                'req_good_set': set(), #set(df.loc[df['importance'] == 1, 'link_id'].to_list()) # list of emails required for coverage - example: 1 per user/company pair\n",
    "                #minus_bad_set=group_minus['bad_set']  # ignored coverage- filtered by minus words\n",
    "                'try_max': 1, # number of tries to build groups in total\n",
    "                'max_len': 1650, # net length of ngrams; for home maximum length of query ~1950 including minus words\n",
    "                'target_good_cover': 0.95, # 0.9995 # target coverage+\n",
    "                'cut_bad_cover': 0.35, # 0.4 # target cutting of the coverage- for each group\n",
    "                'step_bad_cover': 0.10, # step of target bad cutting\n",
    "                'target_good_inc': 2, # lower limit on the increase in coverage+, for example, add at least 3 letters+ per ngram in the first group, 2 in the second...\n",
    "                'user_comp_imp': 0, #int(total_amount_clusters['good_types'] * 0.01) # importance of matching coverage+ as ~1% good coverage for 1 group, for subsequent * 1.5 // time of iter x2\n",
    "                'x_user_comp': False, # mode in which the group is completed when all user/company matchings are covered\n",
    "                'x_tfidf': True # use tf-idf score // time of iter x2\n",
    "                }\n",
    "\n",
    "base_query = 'in:anywhere (\"social security number\"|\"ssn\"|\"soc sec num\"|\"Social Security\"|\"Social Sec\"|\"Soc Sec\"|\"Soc Security Number\"|\"Social Security No\"|\"SS No\"|\"SS Num\"|\"SS Number\"|\"Social Security Administration\"|\"SSA\"|\"Social Sec Admin\"|\"Social Security Admin\"|\"Soc Sec Admin\"|\"Soc Security Administration\"|\"Soc Security Admin\"|\"SS Administration\") '\n",
    "\n",
    "x_test = True\n",
    "\n",
    "df, calc_good, calc_bad, group_minus, groups, query, out_good_set = pipeline_of_query_creating(path_params, ngram_params, minus_params, group_params, base_query, x_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
